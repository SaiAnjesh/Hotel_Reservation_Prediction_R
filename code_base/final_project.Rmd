---
title: "Final Project"
author: "Sai Anjesh"
date: "2024-04-16"
output: html_notebook
---

# HOTEL RESERVATION CANCELLATION PREDCITION


```{r}
## Importing Necessary Libraries
##---------
FUNCTIONS_PATH = "D:/UIS/Studies/Semester4/CSC532/Assignments/ML_Final_Project/ml_final_project/code_base/custom_functions.R"
##---------
source(FUNCTIONS_PATH)
##---------
library(dplyr)
library(ROCR)
library(caret)
library(glmnet)
library(randomForest)
library(gbm)
library(keras)
library(tfruns)
library(abind)
library(mltools)
library(data.table)
library(pROC)
library(ggplot2)
library(reshape2)
library(tidyr)
##---------
```


```{r}
# I am creating this to save models
# Most of the models take time to run
# Can uuse these model objects later during the process

# CHAT_GPT Assistance: how to create folder if it does not exist in R?
# Specify the path to the folder you want to create
models_folder_path <- "D:/UIS/Studies/Semester4/CSC532/Assignments/ML_Final_Project/ml_final_project/ALL_MODELS"
# over_write_models <- T
over_write_models <- F

# Check if the folder exists
if (!file.exists(models_folder_path)) {
  # If the folder does not exist, create it
  dir.create(models_folder_path)
  cat("Folder created successfully!\n")
} else {
  cat("Folder already exists!\n")
}
```


#### Let us import dataset

```{r}
## File Paths
input_folder_path = "D:/UIS/Studies/Semester4/CSC532/Assignments/ML_Final_Project/ml_final_project/input_datasets/"
input_file_name = "hotel_reservations_dataset/Hotel Reservations.csv"
hotel_reservation_data = paste(input_folder_path, input_file_name, sep = "")
```

```{r}
hotel_reservation_data <- read.csv(file = hotel_reservation_data, header = TRUE)
```

#### Let us check some basic information

```{r}
## Primitive Information
# Let us check the shape of the data
print(paste("Number of Records : ", dim(hotel_reservation_data)[1]))
print(paste("Number of Features : ", dim(hotel_reservation_data)[2]))

# Let us check the feature name snad the structure
str(hotel_reservation_data)

# Check the summary
summary(hotel_reservation_data)
```

#### Interesting to see that there are cases of $0 price for certain cases let us look into this section of the data
```{r}
# For EDA
hotel_reservation_data_EDA <- hotel_reservation_data
hotel_reservation_data$booking_cancellation <- ifelse((hotel_reservation_data$booking_status == "Canceled"), T, F)
hotel_reservation_data_EDA$Date <- as.Date(paste(hotel_reservation_data_EDA$arrival_year, 
                                                 hotel_reservation_data_EDA$arrival_month, 
                                                 hotel_reservation_data_EDA$arrival_date, sep = "-"),
                                           format = "%Y-%m-%d")

# Print the result
# print(hotel_reservation_data_EDA$Date)
print(nrow(hotel_reservation_data_EDA))
print(length(unique(hotel_reservation_data_EDA$Date)))
print(summary(hotel_reservation_data_EDA$Date))

# Understand the dates
hotel_reservation_data_EDA_NA_Dates <- hotel_reservation_data_EDA[is.na(hotel_reservation_data_EDA$Date), 
                                                                  c("arrival_year", "arrival_month",
                                                                    "arrival_date", "Date",
                                                                    "avg_price_per_room")]
dates_orig_data <- hotel_reservation_data_EDA[,c("arrival_year", "arrival_month","arrival_date", 
                                                 "Date",
                                                 "avg_price_per_room")]
```
#### What we can understand here is there are just data poitns for 1.5 (From July 2017 to Dec 2018) years and very funny we have 29th feb date in year 2018. Yeah, I know right!? How can Feb have 29 days in 2018. Fun fact: Prices have been alloted for these dates. 

#### This is either data entry problem or data loss / corruption problem.

```{r}
# Let us check date before and post 29 Feb 2018
dates_orig_data_grp <- (dates_orig_data %>% 
                          group_by(Date) %>% 
                          summarise(total = n()) %>% 
                          filter(Date == as.Date("2018-02-28") | Date == as.Date("2018-03-01")))
print(dates_orig_data_grp)
```


#### We see that there are only 61 records in Mar 1 2018 and 165 Records in Feb 28. So, I am gonna update the Feb 29 value as Mar 1 to be fair.

```{r}
# Updating the input dataset
filter_condition = ((hotel_reservation_data$arrival_date == 29) & 
                      (hotel_reservation_data$arrival_month == 2) & 
                      (hotel_reservation_data$arrival_year == 2018))
hotel_reservation_data[filter_condition, "arrival_date"] = 1
hotel_reservation_data[filter_condition, "arrival_month"] = 3


print(summary(
  as.Date(paste(hotel_reservation_data$arrival_year, 
                hotel_reservation_data$arrival_month, 
                hotel_reservation_data$arrival_date, sep = "-"),
          format = "%Y-%m-%d")))
```
So there are no NAs and 549 unique values.

Now let us understand the price againfdt the date and cancellation feature.

```{r}
# For EDA
hotel_reservation_data_EDA <- hotel_reservation_data
```


```{r}
# Price Data Summarization
price_summary_data <- hotel_reservation_data_EDA
price_summary_data$Date <- as.Date(paste(hotel_reservation_data_EDA$arrival_year, 
                                                 hotel_reservation_data_EDA$arrival_month, 
                                                 hotel_reservation_data_EDA$arrival_date, sep = "-"),
                                           format = "%Y-%m-%d")
price_summary_data <- ( 
  price_summary_data %>% 
    select(all_of(c("arrival_year", "arrival_month", "arrival_date", "Date","avg_price_per_room", "booking_cancellation"))) %>% 
    group_by(Date) %>%
    summarise(price_avg = mean(avg_price_per_room),
              price_min = min(avg_price_per_room),
              price_max = max(avg_price_per_room),
              number_book_cancelled = sum(booking_cancellation)))
# Anther Date Column: to check what all dates have no price points

date_check_data <- data.frame(Date = seq(as.Date("2017-07-01"), as.Date("2018-12-31"), by = "day"))
date_check_data$all_dates_flag <- 1

price_summary_data <- merge(x = price_summary_data, y = date_check_data, by = "Date", all.x = T)

# Let us check if we have missed any date time
print("Number of dates that have no price points : ")
print(nrow((price_summary_data[(price_summary_data$all_dates_flag != 1), ])))
```

Since it is zero, I believ we have all the dates in the 1.5 years period and let us see the price points graph
Let us further inspact the date& price column.

```{r}

# Let us create a dataframe that and plot relevant graphs
# https://stackoverflow.com/questions/17150183/plot-multiple-lines-in-one-graph
# https://www.statology.org/ggplot2-multiple-lines/
mdf = melt(price_summary_data, id.vars = c("Date"), measure.vars = c("price_avg", "price_min", "price_max"))
mdf_cancel = melt(price_summary_data, id.vars = c("Date"), measure.vars = c("number_book_cancelled"))
mdf_cancel_2018 = mdf_cancel %>% filter(year(Date) == 2018)
mdf_2017 = mdf %>% filter(year(Date) == 2017)
mdf_2018 = mdf %>% filter(year(Date) == 2018)

```

```{r}

# Let us check the 
price_points_2017 <- (ggplot(data = mdf_2017, aes(x=Date, y=value, group = variable, colour = variable)) + geom_line())
price_points_2018 <- (ggplot(data = mdf_2018, aes(x=Date, y=value, group = variable, colour = variable)) + 
                        geom_line() + 
                        theme(legend.position = "bottom") + 
                        ylab("Price $"))
print(price_points_2018)
```
#### Let us ccheck for the year 2018 and understand the price points. 2018 year is selected because we have one yera of data. There are cases with price points is 0.  We will have to understand that.

```{r}
# Price Points 0
price_less_than_1 <- hotel_reservation_data_EDA %>% filter(avg_price_per_room == 0)
table(price_less_than_1$booking_cancellation)
# so only few cases have been cancelled

# Let us check the market segment that has price $0
table(price_less_than_1$market_segment_type)
```
#### So what we can understand the market segment is either complementary or online. So selected customers could have some offers and have utilised the offer. hence $0 price point. Also, we don't see many cancellations, which also makes sense, whol would want to miss free booking. These are our thoughts.

```{r}
cancellation <- (ggplot(data = mdf_cancel_2018, aes(x = Date, y = value, fill = variable)) +
                   geom_bar(stat = "identity") +
                   labs(x = "Date", y = "Value", fill = "Variable") +
                   scale_x_date(date_labels = "%Y-%m-%d", date_breaks = "1 month") +
                   theme(legend.position = "bottom", axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, size = 10)))

print(cancellation)
```
We can see that there are many booking cancellations are during the months of September, October, Noveber.

##### Let us check the NA values

```{r}
# Let us check the NA cases
# Check if there are missing values
# And let us do a thorough check
sum(is.na(hotel_reservation_data))
colSums(is.na(hotel_reservation_data))

# Check 1
na_records <- hotel_reservation_data[!complete.cases(hotel_reservation_data), ]
```


Fortunately we don't have NA values, we shall continue to transform the structure.

#### Let us convert the struture of the dataframe
```{r}
# Check the str
str(hotel_reservation_data)

categorized_features <- categorize_features(hotel_reservation_data)
# Checking the column names
cat("Numeric columns: \n------\n", paste(categorized_features$numeric_cols, collapse = ", "), "\n")
cat("Factor columns: \n------\n", paste(categorized_features$factor_cols, collapse = ", "), "\n")
cat("Character columns: \n------\n", paste(categorized_features$character_cols, collapse = ", "),"\n")
```

```{r}
# Let us check the character variables and understand the unique values
for(col_val in categorized_features$character_cols) {
  cat(paste("-----", col_val, "-----\n", sep = ""))
  if(col_val == "Booking_ID") {
    print("Uniq Identifier - No need to check the unique values")
  } else {
    cat(paste(sort(unique(hotel_reservation_data[[col_val]]), decreasing = F), collapse = ", "), "\n")
  }
}
```


```{r}
# Removing the first Column 
# Jsut a unique identifier
hotel_reservation_data$Booking_ID <- NULL

# Final OutCome column - Modified
# I We are chaging hte outcome variable, for simplfied understanding of the model and data
# We are more interesetd in understanding people who would end up canceling a booked room.
hotel_reservation_data$booking_cancellation <- ifelse((hotel_reservation_data$booking_status == "Canceled"), "yes", "no")
hotel_reservation_data$booking_cancellation <- factor(hotel_reservation_data$booking_cancellation)
hotel_reservation_data$booking_status <- NULL

hotel_reservation_data$market_segment_type <- factor(hotel_reservation_data$market_segment_type)
hotel_reservation_data$type_of_meal_plan <- factor(hotel_reservation_data$type_of_meal_plan)
hotel_reservation_data$room_type_reserved <- factor(hotel_reservation_data$room_type_reserved)

```

```{r}
# Check the str
str(hotel_reservation_data)

categorized_features <- categorize_features(hotel_reservation_data)
# Checking the column names
cat("Numeric columns: \n------\n", paste(categorized_features$numeric_cols, collapse = ", "), "\n")
cat("Factor columns: \n------\n", paste(categorized_features$factor_cols, collapse = ", "), "\n")
cat("Character columns: \n------\n", paste(categorized_features$character_cols, collapse = ", "),"\n")
```

## NOTE: WE HAVE SKIPPED THE EDA PART....WE WILL HAVE TO COMPLETE IT

```{r}
# Defining Target Variable
target_DV <- "booking_cancellation"
```


```{r}
# Checking the association of variables with target variable

assoc_num_df <- association_numerical_categorical(df = hotel_reservation_data,
                                                  numer_vars = categorized_features$numeric_cols,
                                                  target_var = target_DV)

assoc_fac_df <- association_categorical_categorical(df = hotel_reservation_data,
                                                    factor_vars = categorized_features$factor_cols,
                                                    target_var = target_DV)

qualified_features_data <- assoc_num_df %>% rbind.data.frame(assoc_fac_df)

# CHATGPT: how to sort a dataframe in R?
qualified_features_data[order(qualified_features_data$result, decreasing = FALSE), ]
```

#### We have some variables that are periodic in nature. Date, month, and day. I believe the better way to represent these variables is to map each value into a point in a circle where the lowest value appears next to the largest value in the circle.

```{r}
# Let us convert periodic columns
circular_columns <- c("arrival_date", "arrival_month")


for (col_val in circular_columns) {
  
  x_col_name <- paste(col_val, "_x", sep = "")
  y_col_name <- paste(col_val, "_y", sep = "")
  
  hotel_reservation_data[[x_col_name]] <- cos((2 * pi * hotel_reservation_data[[col_val]]) / max(hotel_reservation_data[[col_val]]))
  hotel_reservation_data[[y_col_name]] <- sin((2 * pi * hotel_reservation_data[[col_val]]) / max(hotel_reservation_data[[col_val]]))
}

# Removing the modified columns
hotel_reservation_data <- hotel_reservation_data %>% select(-one_of(circular_columns))

# Year column into factor
hotel_reservation_data$arrival_year <- factor(hotel_reservation_data$arrival_year)
```

```{r}
# Check the str
str(hotel_reservation_data)
```

Let the outcome column be the final variable in the dataframe. Let sus chage its position.
```{r}
# Cahging the position of the outcome variable
book_cancel <- hotel_reservation_data$booking_cancellation
hotel_reservation_data$booking_cancellation <- NULL
hotel_reservation_data$booking_cancellation <- book_cancel
rm(book_cancel)
```


```{r}
# Check the str
str(hotel_reservation_data)

categorized_features <- categorize_features(hotel_reservation_data)
# Checking the column names
cat("Numeric columns: \n------\n", paste(categorized_features$numeric_cols, collapse = ", "), "\n")
cat("Factor columns: \n------\n", paste(categorized_features$factor_cols, collapse = ", "), "\n")
cat("Character columns: \n------\n", paste(categorized_features$character_cols, collapse = ", "),"\n")
```

#### Let us get started with the modeling

```{r}
# Let us check the class blance
prop.table(table(hotel_reservation_data$booking_cancellation))
```
From the above classes table we can surely say that the 

```{r}
# Let us bacup saiing a data
hotel_reservation_data_processed_bkup <- hotel_reservation_data
```


```{r}
# Let us split the data into train and test
# We are using 90% train and 10% test
# We have over 30K reords
train_index <- createDataPartition(y = hotel_reservation_data$booking_cancellation, p = 0.9, list = FALSE)
hotel_reservation_data_train <- hotel_reservation_data[train_index, ]
hotel_reservation_data_test <- hotel_reservation_data[-train_index, ]
```


```{r}
# Let us determiine the class weights
class_weights_values <- class_weight_determination_fn(output_var = hotel_reservation_data_train$booking_cancellation)
class_weights_vector <- ifelse(hotel_reservation_data_train$booking_cancellation == "yes", 
                               class_weights_values$weight_yes, class_weights_values$weight_no)

```


#### Before deep dive into sophisticated modeling techniques, let us perform some simple heuristic calculation (AKA: Heuristic Model) to understand the data

Why did we select this variable? Usually reservation is cancelled only if booked. (Your reaction : Obviously) There are several variables in that can affect the cancellation but from a receptionist / owner point of view if there is cancellation after several days it gets really frustrating. Hence this is feature to concentrate. 

```{r}
# To understand the possibility of the reservation getting cancelled
# We have to understand the nummber of days between the date of booking and the arrival date
plot(hotel_reservation_data_train$lead_time ~ hotel_reservation_data_train$booking_cancellation, 
     col="orange", 
     main = paste("Box Plot - lead_time vs. booking_cancellation", sep = ""),
     xlab = "Cancellation",
     ylab = "Lead Time",
     horizontal = TRUE)

# For "yes" category
summary_chk = summary(hotel_reservation_data_train[hotel_reservation_data_train$booking_cancellation == "yes", "lead_time"])


```

From the above graph we can understand that the if the lead time crosses 100 days, then there is huge possibility of the room reservation getting cancelled. Hence, we are going to use the median value to classfy the value.



Let us create becnh mark model by assessing the train data
```{r}
# Let us create separate train and test data for bench marking model
hotel_reservation_data_train_BENCHMARK <- hotel_reservation_data_train
hotel_reservation_data_test_BENCHMARK <- hotel_reservation_data_test

# Let us perform our prediction (or say it a guess work... :D)
hotel_reservation_data_test_BENCHMARK <- hotel_reservation_data_test_BENCHMARK %>%  mutate(
  prediction = ifelse(hotel_reservation_data_test_BENCHMARK$lead_time >= summary_chk[["Median"]], "yes", "no"))
hotel_reservation_data_test_BENCHMARK$prediction <- factor(hotel_reservation_data_test_BENCHMARK$prediction)
```

```{r}
# Evaluaition of Benchamrk model
auc_result <- auc(roc(as.numeric(hotel_reservation_data_test_BENCHMARK$booking_cancellation),
                      as.numeric(hotel_reservation_data_test_BENCHMARK$prediction)))

conf_mat <- confusionMatrix(hotel_reservation_data_test_BENCHMARK$prediction, 
                            hotel_reservation_data_test_BENCHMARK$booking_cancellation, 
                            positive = "yes", 
                            mode="everything")

auc_result
conf_mat
```

# Modeling

```{r}
# Variable to contain the list of models
MODEL_LIST <- list()

# Control Variables
MODEL_K_FOLDS <- 10
# MODEL_K_FOLDS <- 5
MODEL_CONTROL_METHOD <- "cv"
# MODEL_TRAIN_METHOD <- "gbm"
MODEL_METRIC <- "ROC"
MODEL_TRAIN_DATA <- hotel_reservation_data_train
MODEL_TEST_DATA <- hotel_reservation_data_test
MODEL_DV_VAR <- "booking_cancellation"
##-------
ALPHA_VALUE = list(
  LASSO = 1,
  RIDGE = 0,
  ELASTIC = seq(0, 1, length = 10)
)
LAMBDA_GRID <- 10^seq(-3, 3, length = 100)
```

```{r}
set.seed(1)
control_values <- trainControl(method = MODEL_CONTROL_METHOD, 
                               number = MODEL_K_FOLDS, 
                               classProbs = TRUE, 
                               summaryFunction = twoClassSummary, 
                               verboseIter = TRUE)
``` 


```{r}
if(over_write_models) {
  start_time <- Sys.time()
  print("##########--------------------LASSO LOGISTIC REGRESSION--------------------##########")
  print("Running Model...")
  
  set.seed(1)
  

  grid_lasso <- expand.grid(alpha = ALPHA_VALUE$LASSO, lambda = LAMBDA_GRID)
  
  LASSO_MODEL <- train(booking_cancellation ~.,
                       data = MODEL_TRAIN_DATA,
                       method = "glmnet",
                       weights = class_weights_vector,
                       metric = MODEL_METRIC,
                       trControl = control_values,
                       tuneGrid = grid_lasso,
                       verboseIter = TRUE,
                       verbose = TRUE)
  
  # print(LASSO_MODEL)
  print(coef(LASSO_MODEL$finalModel, LASSO_MODEL$bestTune$lambda))
  
  # Savig the model objects for future use
  MODEL_NAME <- "LASSO_MODEL"; MODEL_LIST[[MODEL_NAME]] <- LASSO_MODEL;
  saveRDS(MODEL_LIST[[MODEL_NAME]], file = paste(models_folder_path,"/", MODEL_NAME,"_MODEL.rds",sep = ""))
  
  # End time
  end_time <- Sys.time()
  time_difference_minutes <- as.numeric(difftime(end_time, start_time, units = "mins"))
  print(time_difference_minutes)
}

```


```{r}
# 2nd MOdel MOdel - RF Model
# Descision tree is sensitive class imbalance
# Resusing the codes from the lectures
if(over_write_models) {
  start_time <- Sys.time()
  print("##########--------------------RANDOM FOREST--------------------##########")
  print("Running Model...")

set.seed(1)

# grid_rf <- expand.grid(mtry= c(2))
grid_rf <- expand.grid(mtry= c(2, 4, 8))

RANDOM_FOREST_MODEL <- train(  booking_cancellation ~.,
                               data = MODEL_TRAIN_DATA,
                               method = "rf",
                               weights = class_weights_vector,
                               metric = MODEL_METRIC,
                               trControl = control_values,
                               tuneGrid = grid_rf,
                               importance = TRUE,
                               verbose = TRUE)

print(RANDOM_FOREST_MODEL)

# Savig the model objects for future use
MODEL_NAME <- "RANDOM_FOREST_MODEL"; MODEL_LIST[[MODEL_NAME]] <- RANDOM_FOREST_MODEL;
saveRDS(MODEL_LIST[[MODEL_NAME]], file = paste(models_folder_path,"/", MODEL_NAME,"_MODEL.rds",sep = ""))

# End time
  end_time <- Sys.time()
  time_difference_minutes <- as.numeric(difftime(end_time, start_time, units = "mins"))
  print(time_difference_minutes)

}

```


```{r}
# Undersanding the important features
var_importance_RFM <- varImp(RANDOM_FOREST_MODEL)

# Let us check the features
print(var_importance_RFM)
plot(var_importance_RFM)

# HAT GPT ASSISTANCE : TO the get the variable importance plot in pdf file
png(filename = "variable_importance.png")
plot(var_importance_RFM, width = 10, height = 10)
while (!is.null(dev.list()))  dev.off()
```


```{r}
# First model
if(over_write_models) {
  
  start_time <- Sys.time()
  print("##########--------------------GRADIENT BOOST--------------------##########")
  print("Running Model...")
  
  set.seed(1)
  # Automatice Tuning
  GRADIENT_BOOST_MODEL <- train(booking_cancellation ~ ., 
                                data = MODEL_TRAIN_DATA, 
                                method = "gbm", 
                                weights = class_weights_vector,
                                metric = MODEL_METRIC, 
                                trControl = control_values,
                                verbose = TRUE)
  print(GRADIENT_BOOST_MODEL)
  
  # Savig the model objects for future use
  MODEL_NAME <- "GRADIENT_BOOST_MODEL"; MODEL_LIST[[MODEL_NAME]] <- GRADIENT_BOOST_MODEL;
  saveRDS(MODEL_LIST[[MODEL_NAME]], file = paste(models_folder_path,"/", MODEL_NAME,"_MODEL.rds",sep = ""))
  
  
  # End time
  end_time <- Sys.time()
  time_difference_minutes <- as.numeric(difftime(end_time, start_time, units = "mins"))
  print(time_difference_minutes)
}
```



```{r}
# gbm_predictions_prob = predict(m_weighted_gbm, MODEL_TEST_DATA, type="prob")
# gbm_predictions = prediction(predictions = gbm_predictions_prob$yes, MODEL_TEST_DATA$booking_cancellation)
# 
# performance(gbm_predictions, measure = "auc")@y.values
# 
# gbm_predicted_labels = predict(m_weighted_gbm, MODEL_TEST_DATA)
# confusionMatrix(gbm_predicted_labels, MODEL_TEST_DATA$booking_cancellation, positive="yes", mode="everything")
```



```{r}
# Fourth Model
# Set seed - for reproducibility
if(over_write_models) {
  
  start_time <- Sys.time()
  print("##########--------------------SVM LINEAR--------------------##########")
  print("Running Model...")
set.seed(1)

# Define Train Control for SVM
train_control_SVM <- trainControl(method = MODEL_CONTROL_METHOD, 
                              number = MODEL_K_FOLDS, 
                              classProbs = TRUE, 
                              summaryFunction = twoClassSummary, 
                              preProc = c("center", "scale"),
                              verboseIter = TRUE)

# Set Up MOdel

grid_svm <- expand.grid(C = c(0.25, 0.5, 1))

SVM_LINEAR_MODEL <- train( booking_cancellation ~.,
                           data = MODEL_TRAIN_DATA,
                           method = "svmLinear",
                           weights = class_weights_vector,
                           metric = MODEL_METRIC,
                           trControl = train_control_SVM,
                           tuneGrid = grid_svm,
                           verboseIter = TRUE,
                           verbose = TRUE)

# print(SVM_LINEAR_MODEL)

# Savig the model objects for future use
  MODEL_NAME <- "SVM_LINEAR_MODEL"; MODEL_LIST[[MODEL_NAME]] <- SVM_LINEAR_MODEL;
  saveRDS(MODEL_LIST[[MODEL_NAME]], file = paste(models_folder_path,"/", MODEL_NAME,"_MODEL.rds",sep = ""))

# End time
  end_time <- Sys.time()
  time_difference_minutes <- as.numeric(difftime(end_time, start_time, units = "mins"))
  print(time_difference_minutes)
}

```

```{r}
# Fourth Model - SVM RADIAL
# Set seed - for reproducibility
if(over_write_models) {
  
  start_time <- Sys.time()
  print("##########--------------------SVM RADIAL--------------------##########")
  print("Running Model...")
set.seed(1)

# Define Train Control for SVM
train_control_SVM <- trainControl(method = MODEL_CONTROL_METHOD, 
                              number = MODEL_K_FOLDS, 
                              classProbs = TRUE, 
                              summaryFunction = twoClassSummary, 
                              preProc = c("center", "scale"),
                              verboseIter = TRUE)

# Set Up MOdel - Auto Tuning
SVM_RADIAL_MODEL <- train( booking_cancellation ~.,
                           data = MODEL_TRAIN_DATA,
                           method = "svmRadial",
                           weights = class_weights_vector,
                           metric = MODEL_METRIC,
                           trControl = train_control_SVM,
                           verboseIter = TRUE,
                           verbose = TRUE)

# print(SVM_RADIAL_MODEL)

# Savig the model objects for future use
  MODEL_NAME <- "SVM_RADIAL_MODEL"; MODEL_LIST[[MODEL_NAME]] <- SVM_RADIAL_MODEL;
  saveRDS(MODEL_LIST[[MODEL_NAME]], file = paste(models_folder_path,"/", MODEL_NAME,"_MODEL.rds",sep = ""))

# End time
  end_time <- Sys.time()
  time_difference_minutes <- as.numeric(difftime(end_time, start_time, units = "mins"))
  print(time_difference_minutes)
}

```


```{r}
# # Pausing ----
MODEL_LIST <- list("LASSO_MODEL" = LASSO_MODEL 
                   ,"RANDOM_FOREST_MODEL" = RANDOM_FOREST_MODEL,
                   "GRADIENT_BOOST_MODEL" = GRADIENT_BOOST_MODEL,
                   "SVM_LINEAR_MODEL" = SVM_LINEAR_MODEL,
                   "SVM_RADIAL_MODEL" = SVM_RADIAL_MODEL
                   )
models_evaluation_data <- data.frame()
# Models Evaluation
for(model_select in names(MODEL_LIST)) {
  cat(paste("####-----",model_select,"-----####\n", sep = ""))
  model_eval_res <- (classification_evaluation_fn(model_selected_var = MODEL_LIST[[model_select]],
                                                       test_data_selected = MODEL_TEST_DATA,
                                                       focus_outcome = MODEL_DV_VAR))
  interim_data <- data.frame(
    Model_Name = model_select,
    AUC_Value = model_eval_res$AUC_Value,
    Accuracy = model_eval_res$Confusion_Matrix$overall[["Accuracy"]]
  )
  models_evaluation_data <- rbind(interim_data, models_evaluation_data)
  
    
}

models_evaluation_data <- models_evaluation_data %>% arrange(desc(AUC_Value), desc(Accuracy))
```


# NN MOdeling
#### Let us construct the NN for this model
```{r}
# Let us first perfrorm necessary transformation
# Let us first row bind both the train and test dataframe and do one hot encoding
# This 

hotel_reservation_data_train_bkup <- hotel_reservation_data_train
hotel_reservation_data_train_bkup$training_sample <- 1

hotel_reservation_data_test_bkup <- hotel_reservation_data_test
hotel_reservation_data_test_bkup$training_sample <- 0

# Concategnate the dataframes
hotel_reservation_data_combined_1 <- hotel_reservation_data_train_bkup %>% bind_rows(hotel_reservation_data_test_bkup)
rm(hotel_reservation_data_train_bkup, hotel_reservation_data_test_bkup)

# let usf first check the columns for one hot encoding
columns_for_one_hot_encoding <- categorize_features(hotel_reservation_data_combined_1)$factor_cols
columns_for_one_hot_encoding <- setdiff(columns_for_one_hot_encoding, MODEL_DV_VAR)

# Calling custom made one hot One HOt Enccoding 
hotel_reservation_data_combined_2 <- one_hot_encoding_fn(data_val = hotel_reservation_data_combined_1,
                                                         features_to_encode = columns_for_one_hot_encoding)

# Transforming the outcome variable
# Take care of this variable toawrds the end, when you are predicting the outcom based on the model
hotel_reservation_data_combined_2$booking_cancellation <- (
  as.numeric(hotel_reservation_data_combined_2$booking_cancellation) - 1)

# hotel_reservation_data_combined_2[[MODEL_DV_VAR]] <- ifelse(
#   hotel_reservation_data_combined_2[[MODEL_DV_VAR]] == "yes", TRUE, 
#   ifelse(hotel_reservation_data_combined_2[[MODEL_DV_VAR]] == "no", FALSE,
#          hotel_reservation_data_combined_2[[MODEL_DV_VAR]]))


# str(hotel_reservation_data_combined_2)
```


```{r}
# Let us now break the newly updated data into train, test and train into train and validation
hotel_reservation_data_NN_train <- (hotel_reservation_data_combined_2 %>% 
                                      filter(training_sample == 1) %>% 
                                      select(-all_of("training_sample")))
hotel_reservation_data_NN_test <- (hotel_reservation_data_combined_2 %>% 
                                     filter(training_sample == 0) %>% 
                                     select(-all_of("training_sample")))

# Removing this column from the combined datasets
hotel_reservation_data_combined_3 <- hotel_reservation_data_combined_1
hotel_reservation_data_combined_2 <- hotel_reservation_data_combined_2 %>% select(-all_of("training_sample"))
hotel_reservation_data_combined_1 <- hotel_reservation_data_combined_1 %>% select(-all_of("training_sample"))

```

```{r}
# Let us split the train data to train and validation data set
inTrainVal = createDataPartition(hotel_reservation_data_NN_train$booking_cancellation, p=0.9, list=FALSE) 
hotel_reservation_data_NN_train_val   = hotel_reservation_data_NN_train[-inTrainVal,]
hotel_reservation_data_NN_train_train = hotel_reservation_data_NN_train[inTrainVal,]
```


```{r}
# Let us scale the numerical variables
numerical_cols_to_scale <- categorize_features(hotel_reservation_data_combined_1)$numeric_cols
# hotel_reservation_data_combined_1[, numerical_cols_to_scale]

data_train = hotel_reservation_data_NN_train_train
data_val = hotel_reservation_data_NN_train_val
data_test = hotel_reservation_data_NN_test

numerical_features = numerical_cols_to_scale

# Scaling the features
data_train <- data_train %>%  mutate_at(vars(numerical_features), as.numeric)

# Reusig code form the lectures
scaled_data_train <- scale(data_train[, numerical_features])

col_means_train <- attr(scaled_data_train, "scaled:center")
col_stddevs_train <- attr(scaled_data_train, "scaled:scale")

scaled_data_test <- scale(data_test[, numerical_features], center = col_means_train, scale = col_stddevs_train)

scaled_data_val <- scale(data_val[, numerical_features], center = col_means_train, scale = col_stddevs_train)

# Re arragning the features in all the datasets
# Using the train scaling feauters to avoid data leakage
data_train <- cbind(scaled_data_train, data_train[, !(colnames(data_train) %in% numerical_features)])
data_test <- cbind(scaled_data_test, data_test[, !(colnames(data_test) %in% numerical_features)])
data_val <- cbind(scaled_data_val, data_val[, !(colnames(data_val) %in% numerical_features)])

# Returing the data
# Will construct this into a function in fututre project
hotel_reservation_data_NN_train_train = data_train
hotel_reservation_data_NN_train_val = data_val
hotel_reservation_data_NN_test = data_test
```


```{r}
# Dissecting the IDVs and DVs
train_IDV <- as.matrix(hotel_reservation_data_NN_train_train %>% select(-one_of(c(MODEL_DV_VAR))))
train_DV <- (hotel_reservation_data_NN_train_train[, MODEL_DV_VAR])

test_IDV <- as.matrix(hotel_reservation_data_NN_test %>% select(-one_of(c(MODEL_DV_VAR))))
test_DV <- as.matrix(hotel_reservation_data_NN_test[, MODEL_DV_VAR])

val_IDV <- as.matrix(hotel_reservation_data_NN_train_val %>% select(-one_of(c(MODEL_DV_VAR))))
val_DV <- hotel_reservation_data_NN_train_val[, MODEL_DV_VAR]
```

#### Let us create a simple model first and move on to construcutig tuned model, without the class weights and understand the model

```{r}
set.seed(1)

# Simple Model
simple_model <- (
  keras_model_sequential() %>%
    layer_dense(units = 32, activation = "sigmoid", input_shape = dim(train_IDV)[2]) %>%
    layer_dropout(rate = 0.1) %>%
    layer_dense(units = 32, activation = "sigmoid") %>%
    layer_dropout(rate = 0.1) %>%
    layer_dense(units =  1, activation = "sigmoid")
)

simple_model %>% compile(optimizer = optimizer_adam(learning_rate = 0.01),
                         loss = 'binary_crossentropy',
                         metrics = c('accuracy'))

# Checkign the model structure
print(simple_model)

# Trainging the model
history <- simple_model %>% fit(
  x = train_IDV,
  y = train_DV,
  epochs = 10,
  batch_size = 16, 
  validation_data = list(val_IDV, val_DV),
  
  verbose = 1
)

# class_weight = list("0" = class_weights_values$weight_no, "1" = class_weights_values$weight_yes),

history

plot(history)

# Prediction
predicted_labels <- as.numeric(simple_model %>% predict(test_IDV) %>% k_argmax())
cross_table <- table(Actual_Labels = test_DV, Predicted_Labels = predicted_labels)
print(cross_table)


```

The above model does not seem to satify our expectations, so let us contruct a better neural network model.

```{r}
# Performing multiple TF runs
# Just using small values
# Running it on my local machine
tuning_path = "D:/UIS/Studies/Semester4/CSC532/Assignments/ML_Final_Project/ml_final_project/code_base/final_project_tuning.R"
nodes_list = c(64, 128, 256, 392, 512)
learning_rates_list = c(0.01, 0.05, 0.001, 0.005)
batch_size_list = c(16, 32, 64, 128)
activations_list = c( "sigmoid",  "relu", "tanh")
sample_perc = 0.03

# Performing tUNED NN
runs <- tuning_run(tuning_path,
                   flags = list(
                     nodes = nodes_list,
                     learning_rate = learning_rates_list,
                     batch_size = batch_size_list,
                     activation = activations_list
                     ),
                   sample = sample_perc)
```



```{r}
# Extracting the TF run information
# Sorting based on the metric loss
runs <- runs[order(runs$metric_val_loss, decreasing = FALSE),]
as.data.frame(runs)
```

```{r}
# Let’s view the summary of the best run
view_run(runs$run_dir[1])
```



```{r}
# Let us conctaente the val and train datasets
all_train_x <- abind(train_IDV, val_IDV, along = 1)
all_train_y <- abind(train_DV, val_DV, along = 1)
str(all_train_x)
str(all_train_y)
```


Let us perform the final model (neural networks)

```{r}
# Tunedd Parameters Values
final_tuned_params <- list(
  "nodes" = 256,
  "batch_size" = 64,
  "activation" = "sigmoid",
  "learning_rate" = 0.01,
  "epochs" = 30
)

```

```{r}

# Final Neural Network model
final_model <- keras_model_sequential()

final_model %>%
  layer_dense(units = final_tuned_params$nodes, activation = final_tuned_params$activation, input_shape = dim(all_train_x)[2]) %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = final_tuned_params$nodes, activation = final_tuned_params$activation ) %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = final_tuned_params$nodes, activation = final_tuned_params$activation ) %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 1, activation = final_tuned_params$activation)

final_model %>% compile(optimizer = optimizer_adam(learning_rate = final_tuned_params$learning_rate), 
                        loss = 'binary_crossentropy',
                        metrics = c('accuracy'))

final_model_compile <- final_model %>% fit(x = all_train_x,
                                           y = all_train_y,
                                           batch_size = final_tuned_params$batch_size,
                                           epochs = final_tuned_params$epochs,
                                           class_weight = list("0" = class_weights_values$weight_no, 
                                                               "1" = class_weights_values$weight_yes),
                                           verbose = 1)
```

Let us evaluate the final model.

```{r}
final_model %>% evaluate(test_IDV, test_DV)
```

```{r}
MODEL_NAME <- "NEURAL_NETWORK_MODEL"; MODEL_LIST[[MODEL_NAME]] <- final_model;
saveRDS(MODEL_LIST[[MODEL_NAME]], file = paste(models_folder_path,"/", MODEL_NAME,"_MODEL.rds",sep = ""))

runs_save_file_path = paste(models_folder_path, "/", sep = "")
runs_save_file_name = paste("NN_RUNS_", format(Sys.time(), "%Y%m%d_%H%M"), ".csv", sep = "")
runs_save_file_name = paste(runs_save_file_path, runs_save_file_name, sep = "")
write.csv(runs, file = runs_save_file_name, row.names = FALSE)
print(paste(runs_save_file_name, " - file saved"))
```


```{r}
predicted_probs = final_model %>% predict(test_IDV)
ann_predictions = prediction(predicted_probs, test_DV)

auc_value <- performance(ann_predictions, measure = "auc")@y.values[[1]]

predicted.labels= factor(ifelse(predicted_probs>0.5, "1", "0"))
conf_mat_NN <- confusionMatrix(predicted.labels, as.factor(test_DV), mode="everything", positive="1")
```

```{r}
interim_data <- data.frame(
  Model_Name = "NEURAL_NETWORK",
  AUC_Value = auc_value,
  Accuracy = conf_mat_NN$overall[["Accuracy"]]
)

models_evaluation_data <- rbind(models_evaluation_data, interim_data)
models_evaluation_data <- models_evaluation_data %>% arrange(desc(AUC_Value), desc(Accuracy))

```

Fianl Model Check
```{r}
# Pringting the best model
print(classification_evaluation_fn(model_selected_var = RANDOM_FOREST_MODEL,
                              test_data_selected = MODEL_TEST_DATA,
                                                       focus_outcome = MODEL_DV_VAR))
```

```{r}
v
```

